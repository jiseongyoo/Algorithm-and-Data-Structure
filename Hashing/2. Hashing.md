# 2. Hashing

## 2.1 Basic Concept
 Hasing is frequently used for INSERT, SEARCH, and DELETE operation of data because of its advantagous time complexity, O(1) under assumptions. The basic idea of hasing is representing data with a single numeric number, called hash key, and using it as an index of array or a key for comparison. When a hash key is used as an index of array, target data are able to be found(SEARCH) in O(1) time complexity since users can randomly access to data in array. In addition to that, INSERT and DELETE operation with array work the same as SEARCH operation. So you can determine where to insert and what to delete in O(1) time complexity. You may wonder how a numeric number represents data. The answer is hash function, which converts data into a hash key, defined by a programmer.

## 2.2 Hash Function
As mentioned in 2.1, hash function converts data into hash key with rules defined by a programmer. Let me give you an example. If you say that the hash key of "Orange", "Apple", "Strawberry" and "Apple-pen" are 1, 2, 3 and 4 respectively, then you can say hash key 3 means "Apple". The point of hash function we are interested in this section is the rules defined by a programmer. Since performance of hashing technique highly depends on hash function, it is very important to define the conversion rules. There are a number of rules that are already standardized or assured its performance, but absolutely you don't have to follow or use them. You can define your own hasing rules for you problems, and there is no perfect idea that can be used for all situations. However, you should be careful when you design hash function because horrible hash functions could incur data collision too often, which is the main issue of hashing technique.

## 2.3 Design of Hash Function
We should understand which hash functions work well or not. Requirements of good hash function are up to applications of hashing, but less duplicated hash key is a common requirement. In some cases, usually security or identity fields, no duplication is required, so very complicated hash functions have been developed and widly being used. However, you don't have to design a complex hash function everytime if your simple hash function works well for you.
In coding tests, the most frequent usage of hashing is finding string among a set of strings. Why? Because you can determine if a string is not the one that we are looking for through comparison of hash key, regardless of the length of string. The same string must at least have the same hash key, therefore you can skip different hash keys very quickly. When you find matching hash key, you somtimes need one more step to make sure that it is the one you are finding, comparing characters in the string one by one, because it could be duplicated hash key, called hash key collision.

## 2.4 Hash Table
Hash table is an array that maps hash key onto values. An hash key of values is used as an index of array. Since data in array can be accessed in time-complex(1), we can find values in time-complexity(1) with hash key. Even if detail implementation depends on the situation and the programmer, I prefer pointer array hash table that refers address of data. A simple example of hash table is in below. This example doesn't handle all of exceptions including hashkey collision, just showing how hash key works and values get stored.

struct PERSON{
  char name[10];
  int age, hashkey_name;
};

PERSON person[100];
PERSON *hash_table[100]

void push_HashTable(PERSON *p){
 hash_table[p.hashkey_name % 100] = p;
}

PERSON *get_HashTable(int hashkey_name){
 return hash_table[hashkey_name % 100];
}

## 2.5 Hash Table Size
The most significant issue of using hash table is hash key collision, which means that different data have the same hash key and they want the same slot at hash table. As solutions for this problem, you can choose either open addressing or chaining technique, but those have limitation of its performance. Before you choose one solution for collision, proper size of hash table should be determined by you in order to decrease probabilty of occurance of hash key collision. From my experience, between two and four times as big as the number of data is recommanded. If hash table size is too big, too much memory space is wasted.
